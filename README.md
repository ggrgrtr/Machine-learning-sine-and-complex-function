# Принципы работы

### Для обучения и работы нейронной сети необходимы:

**->** Функция активации (пороговая, сигмоидная, SoftMax - для вероятностей многоклассовой классификации), определяющая результат работы нейрона по весам и аргументу

**->** Loss функция (Средне-квадратичная MSE, Бинарная/Кросс-энтропия BCE/CE) для высчитывания коэффициента разницы прдсказанных данных и таргетных

**->** Скрытые нейронные слои и выходной слой

**->** Определение тренировочных, валидационных и тестовых входных, выходных данных

**->** Определение кучи (batch), где 1<<batch<<len(train Tensor) и обучение Н.С. кучами

### Уточнения
Функция активации принимает выход нейрона после применения соответсвующего веса w на каждый вход и аргумента b - bias. Для каждой цели своя функция (например сочетание MSE и сигмоидной ф. активации при вычислении производной может привести к паралечу сети)

Функция потерь (F Loss) нужна для настройки весов Н.С.

Самый простой вариант - вычисление Градиентного спуска для нахождения минимума Loss Функции, где **W** = *[w1 w2 ... wn b]*, a - скорость обучения (градиентный шаг)

**W**1 = **W**0 - a*Grad[F(**W**0)]
...

**W**n+1 = **W**n - a*Grad[F(**W**n)]

Недостаток только в том, что Градинетный спуск может найти не оптимальный минимум.

График Loss-функции. **W**i - prediction, **W**j - target
<img width="1278" height="958" alt="image" src="https://github.com/user-attachments/assets/2bfc1205-1e2a-4117-ab38-47414a7b5ce1" />



x1, x2 ... xn -> Neuron[ w1*x1 + w2*x2 + ... + wn*xn +b ] -> Z -> Activation() -> **Prediction**

<img width="766" height="194" alt="image" src="https://github.com/user-attachments/assets/a5ea4ca0-f991-49b6-abfa-1734f54975b5" />



Для реализации Градиентного спуска, устанавления слоев нейронов и обновления весов я использовал библиотеку PyTorch

А для визуализации - matplotlib

Для определения классов вин использовались входной нейронный слой с двумя нейроннами, выходной нейронный слой с 3 нейронами (3 класса вин) и SoftMax от PyTorch для визуализции валидности предсказаний, вычисляющий вероятность наследования объектом конкретного класса. 
Побатчевое (кучей) обучение для повышения точности и скорости (NumPy для перемешивания индексов строк Тензора)

*Длина батч, кол-во скрытых слоев, кол-во нейронов в скрытых слоях, метод градиентного спуска (torch.optimizer) определяются вручную*

*Вся конкретика по коду в комментариях*
